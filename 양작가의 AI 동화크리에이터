{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ivNj-86S5g4z2ktIo-9TXalWVJ89MZBs","timestamp":1669353696214}],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"premium"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kc4ga8D4mu2f","executionInfo":{"status":"ok","timestamp":1669598707943,"user_tz":-540,"elapsed":5,"user":{"displayName":"storynara ai","userId":"01960205458854263666"}},"outputId":"54d57774-5bf8-492c-88c4-c515d2b10c62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Nov 28 01:25:06 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  A100-SXM4-40GB      Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   29C    P0    45W / 400W |      0MiB / 40536MiB |      0%      Default |\n","|                               |                      |             Disabled |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","source":["!pip install -qq transformers accelerate"],"metadata":{"id":"8ghUJtQwnDl4","executionInfo":{"status":"ok","timestamp":1669598729195,"user_tz":-540,"elapsed":9939,"user":{"displayName":"storynara ai","userId":"01960205458854263666"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"383cbaa4-c5c5-4312-a609-a15587746f71"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 5.5 MB 4.7 MB/s \n","\u001b[K     |████████████████████████████████| 175 kB 68.1 MB/s \n","\u001b[K     |████████████████████████████████| 182 kB 91.7 MB/s \n","\u001b[K     |████████████████████████████████| 7.6 MB 89.9 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM \n","\n","tokenizer = AutoTokenizer.from_pretrained(\n","  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n","  bos_token='[BOS]', eos_token='[EOS]', unk_token='[UNK]', pad_token='[PAD]', mask_token='[MASK]'\n",")\n","model = AutoModelForCausalLM.from_pretrained(\n","  'kakaobrain/kogpt', revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n","  pad_token_id=tokenizer.eos_token_id,\n","  torch_dtype='auto', low_cpu_mem_usage=True\n",").to(device='cuda', non_blocking=True)\n","_ = model.eval()\n","\n","prompt = '인공지능아, 너는 말을 할 수 있니?'\n","with torch.no_grad():\n","  tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n","  gen_tokens = model.generate(tokens, do_sample=True, temperature=0.85, max_length=124)\n","  generated = tokenizer.batch_decode(gen_tokens)[0]\n","  \n","print(generated)"],"metadata":{"id":"Lrw2L35Ym2e_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"SED4qL3x5-oG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = '''제목 : 마녀와 거울\n","내용 : 아주아주 먼 옛날 숲속에 마녀가 살고 있었어요. 마녀는 거울을 소중하게 여겼어요. 왜냐하면 거울은 무엇이든 솔직하게 말해주었기 때문이었죠.\n","'''\n","\n","with torch.no_grad():\n","    tokens = tokenizer.encode(prompt, return_tensors='pt').to(device='cuda', non_blocking=True)\n","    gen_tokens = model.generate(tokens, do_sample=True, temperature=0.85, max_length=624)\n","    generated = tokenizer.batch_decode(gen_tokens)[0]\n","  \n","print(generated)"],"metadata":{"id":"mAginC8DUUq2"},"execution_count":null,"outputs":[]}]}